# Decide which parts of the pipeline you want to run (only part 1 (ASD with rttm file output), only part 2 (input is rttm file and output the analysis, csv file), or both)
# 1: ASD and Speaker Diarization (input: video, output rttm file)
# 2: Communication Pattern and Emotion Analysis (input: rttm file, output: csv file)
# 3: Visualize the results (input: csv file, output: plots)
# 4: Inference -> predict the PERMA values
# [1,2,3,4] means run all parts
RUN_PIPELINE_PARTS: [1,2,3,4]

# -- PART 1: THE FOLLOWING CONFIGS ARE ALL USED FOR ACTIVE SPEAKER DETECTION 

# Name of the input file to be processed (.mp4 or .avi) two_min_snippet
VIDEO_NAME: "001"

# Path for the pretrained TalkNet model
PRETRAINED_ASD_MODEL: "pretrain_TalkSet.model"

# Number of workers for ffmpeg (needed for Active Speaker Detection)
N_DATA_LOADER_THREAD: 32

# Scale factor for face detection, the frames will be scale to 0.25 orig
FACE_DET_SCALE: 0.25

# Number of min frames for each track
MIN_TRACK: 25

# Number of missed detections allowed before tracking is stopped (e.g. 25 fps -> 1 sec)
NUM_FAILED_DET: 100

# Minimum face size in pixels
MIN_FACE_SIZE: 1

# Scale bounding box
CROP_SCALE: 0.40

# To speed up the face tracking, we only track the face in every x frames
FRAMES_FACE_TRACKING: 2

# The start time of the video
START: 0

# The duration of the video, when set as 0, will extract the whole video
DURATION: 0

# Enable or disable multiprocessing for the active speaker detection (disable if you face errors)
MULTIPROCESSING: True

# Number of calculated embeddings per person to match the faces of the same person (the more the better, but also more storage necessary)
N_EMBEDDINGS: 10

# If two face tracks (see folder faces_id) belong together (based on cosine similarity)
THRESHOLD_SAME_PERSON: 0.30

# If enabled, it will create a video for each track, where only the segments where the person is speaking are included
CREATE_TRACK_VIDEOS: False

# If enabled, it will create a video where you can see the speaking person highlighted (e.g. used for debugging)
INCLUDE_VISUALIZATION: False

# -- PART 2: THE FOLLOWING CONFIGS ARE ALL USED FOR Analysing the audio files

# This is the amount of seconds in which the audio is split and analyzed (300s means in chunks of 5 min)
# Note: If the video length is shorter than the unit of analysis, then this number will be set to video length / 2  
UNIT_OF_ANALYSIS: 300

# -- PART 4: THE FOLLOWING CONFIGS ARE ALL USED FOR Inferencing the PERMA values

# Chossing the scale of the final PERMA values (either "norm": 0 - 1, or "defaul": 0 - 7)
PERMA_SCALE: "norm"