# Decide which parts of the pipeline you want to run (only part 1 (ASD with rttm file output), only part 2 (input is rttm file and output the analysis, csv file), or both)
# 1: ASD and Speaker Diarization (input: video, output rttm file)
# 2: Communication Pattern and Emotion Analysis (input: rttm file, output: csv file)
# 3: Visualize the results (input: csv file, output: plots)
# [1,2,3] means run all parts
RUN_PIPELINE_PARTS: [1]

# -- PART 1: THE FOLLOWING CONFIGS ARE ALL USED FOR ACTIVE SPEAKER DETECTION 

# Name of the input file to be processed (.mp4 or .avi)
VIDEO_NAME: "001"

# Path for the pretrained TalkNet model
PRETRAINED_ASD_MODEL: "pretrain_TalkSet.model"

# Number of workers for ffmpeg (needed for Active Speaker Detection)
N_DATA_LOADER_THREAD: 32

# Scale factor for face detection, the frames will be scale to 0.25 orig
FACE_DET_SCALE: 0.25

# Number of min frames for each scene and track
MIN_TRACK: 25

# Number of missed detections allowed before tracking is stopped (e.g. 25 fps -> 1 sec)
NUM_FAILED_DET: 100

# Minimum face size in pixels
MIN_FACE_SIZE: 1

# Scale bounding box
CROP_SCALE: 0.40

# To speed up the face tracking, we only track the face in every x frames
FRAMES_FACE_TRACKING: 2

# The start time of the video
START: 0

# The duration of the video, when set as 0, will extract the whole video
DURATION: 0

# If two face tracks (see folder pycrop) are close together (-> below that threshold) and are not speaking at the same time, then it is the same person
THRESHOLD_SAME_PERSON: 1.05

# If enabled, it will create a video for each track, where only the segments where the person is speaking are included
CREATE_TRACK_VIDEOS: False

# If enabled, it will create a video where you can see the speaking person highlighted (e.g. used for debugging)
INCLUDE_VISUALIZATION: True

# -- PART 2: THE FOLLOWING CONFIGS ARE ALL USED FOR Analysing the audio files

# This is the amount of seconds in which the audio is split and analyzed (300s means in chunks of 5 min)
UNIT_OF_ANALYSIS: 180